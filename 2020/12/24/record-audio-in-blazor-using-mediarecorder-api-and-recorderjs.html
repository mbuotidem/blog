<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Record Audio In Blazor Using Mediarecorder Api And Recorderjs | Isaac Mbuotidem</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Record Audio In Blazor Using Mediarecorder Api And Recorderjs" />
<meta name="author" content="Isaac Mbuotidem" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="I recently started playing with Blazor and I decided to build an app that needed to get audio recorded from the users microphone via the browser. This post explains how I did that on Server Side Blazor." />
<meta property="og:description" content="I recently started playing with Blazor and I decided to build an app that needed to get audio recorded from the users microphone via the browser. This post explains how I did that on Server Side Blazor." />
<link rel="canonical" href="https://mbuotidem.github.io/blog/2020/12/24/record-audio-in-blazor-using-mediarecorder-api-and-recorderjs.html" />
<meta property="og:url" content="https://mbuotidem.github.io/blog/2020/12/24/record-audio-in-blazor-using-mediarecorder-api-and-recorderjs.html" />
<meta property="og:site_name" content="Isaac Mbuotidem" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-12-24T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"I recently started playing with Blazor and I decided to build an app that needed to get audio recorded from the users microphone via the browser. This post explains how I did that on Server Side Blazor.","url":"https://mbuotidem.github.io/blog/2020/12/24/record-audio-in-blazor-using-mediarecorder-api-and-recorderjs.html","@type":"BlogPosting","headline":"Record Audio In Blazor Using Mediarecorder Api And Recorderjs","dateModified":"2020-12-24T00:00:00-06:00","datePublished":"2020-12-24T00:00:00-06:00","author":{"@type":"Person","name":"Isaac Mbuotidem"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://mbuotidem.github.io/blog/2020/12/24/record-audio-in-blazor-using-mediarecorder-api-and-recorderjs.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://mbuotidem.github.io/blog/feed.xml" title="Isaac Mbuotidem" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Isaac Mbuotidem</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Record Audio In Blazor Using Mediarecorder Api And Recorderjs</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-12-24T00:00:00-06:00" itemprop="datePublished">
        Dec 24, 2020
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Isaac Mbuotidem</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      12 min read
    
</span></p>

    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>I recently started playing with Blazor and I decided to build an app that needed to get  audio recorded from the users microphone via the browser. This post explains how I did that on Server Side Blazor.</p>

<p>I decided to go with using the browser’s <a href="https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices">MediaDevices</a> interface in conjunction with Matt Diamond’s aptly named <a href="https://github.com/mattdiamond/Recorderjs">Recorder.js</a> library. Although Recorder.js is no longer actively maintained, it still works and more importantly, lets me record in <code class="language-plaintext highlighter-rouge">.wav</code> format which was part of my requirement.</p>

<p>So how does all of this work in the context of a Blazor application? My approach boils down to the following:</p>
<ol>
  <li>Record the audio from the user’s browser using JavaScript</li>
  <li>Post the recorded audio blob to an API endpoint on my Blazor Server App using <a href="https://developer.mozilla.org/en-US/docs/Web/API/XMLHttpRequest">XMLHttpRequest</a> and <a href="https://developer.mozilla.org/en-US/docs/Web/API/FormData">FormData</a>.</li>
  <li>Save the audio blob to a file on disk.</li>
</ol>

<h2 id="record-the-audio-from-the-users-browser-using-javascript-and-recorderjs">Record the audio from the user’s browser using JavaScript and Recorder.js</h2>

<p>As mentioned earlier, I’ll be using the MediaDevices API along with Recorder.js to record the audio from the browser. The MediaDevices API has a method, <a href="https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia"><code class="language-plaintext highlighter-rouge">getUserMedia()</code></a> which enables recording of all sorts of media streams - audio, video, screen share etc. In our case, we will be using just the audio recording capability via Recorder.js. More on this in a second.</p>

<h3 id="the-user-interface">The user interface</h3>
<p>The UI for the audio recording is lifted straight from the <a href="https://mdn.github.io/web-dictaphone/">web dictaphone</a> sample.</p>

<p><img src="/blog/images/12_24_20.PNG" alt="Web dictaphone user interface for recording audio from browser" class="img-responsive" /></p>

<p>As the image above shows, we  have a record and stop button along with an HTML5 canvas that visualizes the audio stream being heard by the microphone. Credit for the visualizer code goes to <a href="https://soledadpenades.com/">Soledad Penades</a>.</p>

<p>Here is the Blazor component code for the UI above. I simply replaced the code that was in <code class="language-plaintext highlighter-rouge">Index.razor</code> but feel free to create and use a different component.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@page "/"
@inject IJSRuntime jsRuntime

&lt;div class="wrapper mt-5"&gt;
    &lt;section class="main-controls"&gt;
        &lt;canvas id="canvas" class="visualizer" height="60"&gt;&lt;/canvas&gt;
        &lt;div id="buttons"&gt;
            &lt;button class="@recordButton" disabled="@Recording" @onclick=Record&gt;Record&lt;/button&gt;
            &lt;button class="stop" disabled="@NotRecording" @onclick=Stop&gt;Stop&lt;/button&gt;
        &lt;/div&gt;
    &lt;/section&gt;

    &lt;section class="sound-clips"&gt;
    &lt;/section&gt;
    &lt;audio controls autoplay&gt;
    &lt;/audio&gt;
&lt;/div&gt;


@code{
    string recordButton = "record";

    bool recording = false;
    bool notRecording = true;


    private async Task Record()
    {

        recordButton = "recording";
        recording = true;
        notRecording = false;
        await jsRuntime.InvokeVoidAsync("MyJSMethods.startRecording");
    }

    private async Task Stop()
    {
        recordButton = "record";
        recording = false;
        notRecording = true;
        await jsRuntime.InvokeVoidAsync("MyJSMethods.stopRecording");
    }

}
</code></pre></div></div>

<p>The field <code class="language-plaintext highlighter-rouge">recordButton</code> toggles the class of the record button from ‘record’ to ‘recording’. We use this to change the record button’s color to red when a recording is in progress and back to blue when recording is stopped.</p>

<p>The fields ‘recording’ and ‘notRecording’ are boolean, and are used to enable and disable clicking on the record and stop buttons depending on if a recording is in progress.</p>

<p>The component’s methods are pretty simple. <code class="language-plaintext highlighter-rouge">Record</code> toggles our CSS property fields and then calls out to a JavaScript function <code class="language-plaintext highlighter-rouge">startRecording</code> via the <a href="https://blazor-university.com/javascript-interop/calling-javascript-from-dotnet/">IJSRuntime</a> service that enables Blazor’s JavaScript interoperability. <code class="language-plaintext highlighter-rouge">Stop</code> toggles back the CSS fields and then calls the JavaScript function <code class="language-plaintext highlighter-rouge">stopRecording</code> to, well, stop recording.</p>

<p>Below is the CSS for the component which I placed in <code class="language-plaintext highlighter-rouge">site.css</code>.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#buttons {
    display: flex;
    flex-direction: row;
    justify-content: space-between;
}

    #buttons button {
        font-size: 1rem;
        padding: 1rem;
        width: calc(50% - 0.25rem);
    }


.record {
    background: #0088cc;
    text-align: center;
    color: white;
}

    .record:hover, .record:focus {
        box-shadow: inset 0px 0px 10px rgba(255, 255, 255, 1);
        background: #0ae;
    }

    .record:active {
        box-shadow: inset 0px 0px 20px rgba(0,0,0,0.5);
        transform: translateY(2px);
    }

.recording {
    background: red;
    text-align: center;
    color: white;
}

.stop {
    font-size: 1rem;
    background: #0088cc;
    text-align: center;
    color: white;
    border: none;
    transition: all 0.2s;
    padding: 0.5rem;
}

    .stop:hover, .stop:focus {
        box-shadow: inset 0px 0px 10px rgba(255, 255, 255, 1);
        background: #0ae;
    }

    .stop:active {
        box-shadow: inset 0px 0px 20px rgba(0,0,0,0.5);
        transform: translateY(2px);
    }


</code></pre></div></div>

<p>Now that we understand how our Blazor component works, lets talk about the JavaScript part.</p>

<h3 id="the-javascript---getusermedia-and-recorderjs">The JavaScript - <code class="language-plaintext highlighter-rouge">getUserMedia</code> and <code class="language-plaintext highlighter-rouge">Recorder.js</code></h3>

<p>Our first step is to add a link to Recorder.js in our <code class="language-plaintext highlighter-rouge">_Host.cshtml</code> page, right below the link to the Blazor server framework script. We also include an empty <code class="language-plaintext highlighter-rouge">&lt;script&gt;</code> tag that will contain our JavaScript.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;script src="_framework/blazor.server.js"&gt;&lt;/script&gt;
&lt;script src="https://cdn.rawgit.com/mattdiamond/Recorderjs/08e7abd9/dist/recorder.js"&gt;&lt;/script&gt;
&lt;script&gt;
  
&lt;/script&gt;

</code></pre></div></div>

<p>Next, we can start writing the JavaScript we need to record audio. Let’s start with the functions that are called from C#. Note that the rest of the JavaScript code in this post should be placed within the empty <code class="language-plaintext highlighter-rouge">&lt;script&gt;</code> tag above.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>window.MyJSMethods = {

    startRecording: function () {
        navigator.getUserMedia({ audio: true }, onSuccess, onError);
    },

    stopRecording: function (element) {
        stop.click();
    },
}

</code></pre></div></div>
<p><code class="language-plaintext highlighter-rouge">startRecording</code> above invokes the browser’s <a href="https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia"><code class="language-plaintext highlighter-rouge">getUserMedia</code></a> method which prompts the user to grant us access to their microphone in order to start recording audio. If that request succeeds, then we invoke the <code class="language-plaintext highlighter-rouge">onSuccess</code> method which is where the actual recording takes place. If the user refuses to grant us access, then we call <code class="language-plaintext highlighter-rouge">onError</code>. The nice thing about the way this works is that the user is prompted to grant access only once - the browser will remember that access was granted on subsequent visits to your page. Lets look at <code class="language-plaintext highlighter-rouge">onError</code>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>let onError = function (err) {
    console.log('The following error occurred: ' + err);
};

</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">onError</code> simply logs the error to the console. This was fine by me during development but is not very useful for the end user. Consider improving this to issue an alert and tell the user “Hey, you need to grant us access to your microphone if you want us to record your audio!”. Next up, <a name="onSuccess"><code class="language-plaintext highlighter-rouge">onSuccess</code></a>, where the recording magic happens!</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>let stop = document.querySelector('.stop');

let onSuccess = function (stream) {
    let recorder;
    let context;
    let audio = document.querySelector('audio');
    stop.disabled = false;

    let mainSection = document.querySelector('.main-controls');
    const canvas = document.querySelector('.visualizer');
    canvas.width = mainSection.offsetWidth;

    const canvasCtx = canvas.getContext("2d");

    context = new AudioContext();
    let mediaStreamSource = context.createMediaStreamSource(stream);
    recorder = new Recorder(mediaStreamSource);
    recorder.record();

    //visualize(stream, canvas, canvasCtx);


    stop.onclick = function () {
        recorder.stop();
        
        recorder.exportWAV(function (s) {
            wav = window.URL.createObjectURL(s);
            audio.src = window.URL.createObjectURL(s);
            let filename = new Date().toISOString().replaceAll(':', "");
            let fd = new FormData();
            fd.append("file", s, filename);
            let xhr = new XMLHttpRequest();
            xhr.addEventListener("load", transferComplete);
            xhr.addEventListener("error", transferFailed)
            xhr.addEventListener("abort", transferFailed)
            xhr.open("POST", "api/SaveAudio/Save/", true);
            xhr.send(fd);

        });

        stop.disabled = true;


        function transferComplete(evt) {
            console.log("The transfer is complete.");
            //GLOBAL.DotNetReference.invokeMethodAsync('Recognize', filename);

        }

        function transferFailed(evt) {
            console.log("An error occurred while transferring the file.");

            console.log(evt.responseText);
            console.log(evt.status);
        }

    }
}

</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">onSuccess</code> is called from <code class="language-plaintext highlighter-rouge">getUserMedia</code> which passes it a stream of the audio source. We then create the <code class="language-plaintext highlighter-rouge">recorder</code>, <code class="language-plaintext highlighter-rouge">context</code>, and <code class="language-plaintext highlighter-rouge">audio</code> variables. The API for Recorder.js is intuitive - to record, you call the <code class="language-plaintext highlighter-rouge">record</code> method on a <code class="language-plaintext highlighter-rouge">Recorder</code> object. A <code class="language-plaintext highlighter-rouge">Recorder</code> object takes a source and an optional config as parameters. In our case, we are using the browser’s <a href="https://developer.mozilla.org/en-US/docs/Web/API/AudioContext"><code class="language-plaintext highlighter-rouge">AudioContext</code></a> interface as our source. <code class="language-plaintext highlighter-rouge">AudioContext</code> processes the stream into an audio source that our <code class="language-plaintext highlighter-rouge">Recorder</code> instance can use when we invoke its <code class="language-plaintext highlighter-rouge">record</code> method. The <code class="language-plaintext highlighter-rouge">audio</code> variable just holds a reference to the HTML5 audio element through which we will play back the recorded audio to the user.</p>

<p>We also created two canvas related variables, <code class="language-plaintext highlighter-rouge">canvas</code> and <code class="language-plaintext highlighter-rouge">canvasCtx</code>, which we pass into a commented out call to the <code class="language-plaintext highlighter-rouge">visualize</code> function. This function handles the visualization of the audio stream and is not required for the recording to work. Ignore it for now - we will <a href="#visualizer">circle back</a> to it.</p>

<p>Next is our stopping mechanism. Outside <code class="language-plaintext highlighter-rouge">onSuccess</code>, we have a handle on the stop button. To stop recording, we attach a function to the stop button’s <code class="language-plaintext highlighter-rouge">onclick</code> event which does a couple of things. First, it stops the recording and then uses Recorder.js’s <code class="language-plaintext highlighter-rouge">exportWAV</code> method to export the audio blob as a <code class="language-plaintext highlighter-rouge">.wav</code> file. When the file is ready, we create a filename based on the timestamp. We set this file as the source for our HTML5 audio element to allow for immediate playback of the recorded audio. Then we post the audio file to our backend via <code class="language-plaintext highlighter-rouge">XMLHttpRequest</code>.</p>

<p>Before we discuss how the file is posted to our backend, a quick word about the functions <code class="language-plaintext highlighter-rouge">transferComplete</code> and <code class="language-plaintext highlighter-rouge">transferFailed</code>. You can do whatever you want to based on the status of the transfer of the file via <code class="language-plaintext highlighter-rouge">XMLHttpRequest</code> by attaching event listeners to it. For example, in the actual application I was building, on a successful POST, I invoked a C# method, in the commented out line reproduced below:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>//GLOBAL.DotNetReference.invokeMethodAsync('Recognize', filename);

</code></pre></div></div>

<p>This line triggered a method called Recognize in that version of my Blazor Component to perform speech recognition on the recorded audio file. To learn more about calling C# code from JavaScript via the Blazor JavaScript interop, see <a href="https://docs.microsoft.com/en-us/aspnet/core/blazor/call-dotnet-from-javascript?view=aspnetcore-3.1">here</a> and <a href="https://blazor-university.com/javascript-interop/calling-dotnet-from-javascript/">here</a>.</p>

<h2 id="post-the-recorded-audio-blob-to-an-api-endpoint-on-my-blazor-server-app-using-xmlhttprequest-and-formdata">Post the recorded audio blob to an API endpoint on my Blazor Server App using <code class="language-plaintext highlighter-rouge">XMLHttpRequest</code> and <code class="language-plaintext highlighter-rouge">FormData</code></h2>

<p>Posting the audio blob is fairly straight forward. As mentioned earlier, we use the <a href="https://developer.mozilla.org/en-US/docs/Web/API/XMLHttpRequest">XMLHttpRequest</a> and <a href="https://developer.mozilla.org/en-US/docs/Web/API/FormData">FormData</a> objects. Here’s that portion of the <code class="language-plaintext highlighter-rouge">onSuccess</code> function again:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>recorder.exportWAV(function (s) {
    wav = window.URL.createObjectURL(s);
    audio.src = window.URL.createObjectURL(s);
    let filename = new Date().toISOString().replaceAll(':', "");
    let fd = new FormData();
    fd.append("file", s, filename);
    let xhr = new XMLHttpRequest();
    xhr.addEventListener("load", transferComplete);
    xhr.addEventListener("error", transferFailed)
    xhr.addEventListener("abort", transferFailed)
    xhr.open("POST", "api/SaveAudio/Save/", true);
    xhr.send(fd);
});

</code></pre></div></div>

<p>First we create the <code class="language-plaintext highlighter-rouge">FormData</code> object and then use its <a href="https://developer.mozilla.org/en-US/docs/Web/API/FormData/append"><code class="language-plaintext highlighter-rouge">append</code></a> method to add the filename we created as well as the recorded <code class="language-plaintext highlighter-rouge">.wav</code> file, here represented by <code class="language-plaintext highlighter-rouge">s</code>, to the form. Then we create the XMLHttpRequest object and attach event listeners for a successful transfer, as indicated by <code class="language-plaintext highlighter-rouge">load</code>, as well as for failures such as <code class="language-plaintext highlighter-rouge">error</code> and <code class="language-plaintext highlighter-rouge">abort</code>.</p>

<p>Next, we initialize a <code class="language-plaintext highlighter-rouge">POST</code> request with XMLHttpRequest’s <code class="language-plaintext highlighter-rouge">open</code> method, indicating the API endpoint url <code class="language-plaintext highlighter-rouge">api/SaveAudio/Save</code> as the target. And then we invoke the <code class="language-plaintext highlighter-rouge">send</code> method, performing the actual POST request with the <code class="language-plaintext highlighter-rouge">FormData</code> object containing our audio blob file as its payload. Note that if your API is on a different domain, you might need to take steps to resolve <a href="https://docs.microsoft.com/en-us/aspnet/core/security/cors?view=aspnetcore-5.0">CORS</a> issues.</p>

<h3 id="setup-required-to-create-the-net-web-api-endpoint">Setup required to create the .NET Web API endpoint</h3>

<p>To add an API endpoint to our Blazor Server application, we need to make some changes to the <code class="language-plaintext highlighter-rouge">Startup.cs</code> class. Add the using statement for .Net Core MVC to the list of using statements.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>using Microsoft.AspNetCore.Mvc;

</code></pre></div></div>

<p>Then modify the <code class="language-plaintext highlighter-rouge">ConfigureServices</code> method, adding the Mvc service:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>public void ConfigureServices(IServiceCollection services)
{
    services.AddMvc(options =&gt; options.EnableEndpointRouting = false).SetCompatibilityVersion(CompatibilityVersion.Latest);
    ...
}
</code></pre></div></div>

<p>Next, add <code class="language-plaintext highlighter-rouge">app.UseMvcWithDefaultRoute();</code> to the <code class="language-plaintext highlighter-rouge">Configure</code> method. I placed it right after <code class="language-plaintext highlighter-rouge">app.UseRouting()</code> and before <code class="language-plaintext highlighter-rouge">app.UseEndpoints()</code>;</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>public void Configure(IApplicationBuilder app, IWebHostEnvironment env)
    {
        ...
        app.UseRouting();
        
        app.UseMvcWithDefaultRoute();


        app.UseEndpoints(endpoints =&gt;
        {
            endpoints.MapControllers();
            endpoints.MapBlazorHub();
            endpoints.MapFallbackToPage("/_Host");
        });
    }
</code></pre></div></div>

<p>With this setup in place, we can now create our API controller class.</p>

<h2 id="save-the-audio-blob-to-a-file-on-disk">Save the audio blob to a file on disk.</h2>

<p>To receive the audio, we will be creating an ASP.NET Core Controller with a method <code class="language-plaintext highlighter-rouge">Save</code>. The class itself is called <code class="language-plaintext highlighter-rouge">SaveAudio</code>. Together, these map to the the API endpoint <code class="language-plaintext highlighter-rouge">api/SaveAudio/Save</code> which we used earlier in the JavaScript code. To add this route to our controller, we use the <a href="https://docs.microsoft.com/en-us/aspnet/core/mvc/controllers/routing?view=aspnetcore-5.0">attribute route</a> <code class="language-plaintext highlighter-rouge">[Route("api/[controller]/Save")]</code>. Below is the code.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>using Microsoft.AspNetCore.Http;
using Microsoft.AspNetCore.Mvc;
using System.IO;
using System.Threading.Tasks;


namespace BlazorAudioRecorder
{
    public class SaveAudio: Controller
    {
        Microsoft.AspNetCore.Hosting.IWebHostEnvironment _hostingEnvironment;

        public SaveAudio(Microsoft.AspNetCore.Hosting.IWebHostEnvironment hostingEnvironment)
        {
            _hostingEnvironment = hostingEnvironment;

        }

        [Route("api/[controller]/Save")]
        [HttpPost]
        public async Task&lt;IActionResult&gt; Save(IFormFile file)
        {
            if (file.ContentType != "audio/wav")
            {
                return BadRequest("Wrong file type");
            }
            var uploads = Path.Combine(_hostingEnvironment.WebRootPath, "uploads");
            
            var filePath = Path.Combine(uploads, file.FileName + ".wav");
            using (var fileStream = new FileStream(filePath, FileMode.Create))
            {
                await file.CopyToAsync(fileStream);
            }
            return Ok("File uploaded successfully");
        }
    }
}

</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">Save</code> method will be invoked when an <code class="language-plaintext highlighter-rouge">HttpPost</code> request is made to the endpoint <code class="language-plaintext highlighter-rouge">api/SaveAudio/Save</code>. Notice that the name of the <a href="https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.http.iformfile?view=aspnetcore-5.0"><code class="language-plaintext highlighter-rouge">IFormFile</code></a> parameter is <code class="language-plaintext highlighter-rouge">file</code> which corresponds to the name we gave the audio blob when creating our <code class="language-plaintext highlighter-rouge">FormData</code> object earlier. Here is that specific line of JavaScript code again:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fd.append("file", s, filename);

</code></pre></div></div>

<p>By doing this, we can rely on ASP.NET <a href="https://docs.microsoft.com/en-us/aspnet/core/mvc/models/model-binding?view=aspnetcore-5.0">model binding</a> magic to match everything up for us. The rest of the code is fairly simple, we check if the content type matches our expectation of <code class="language-plaintext highlighter-rouge">audio/wav</code>. If it doesn’t, we reject it, but if does, we go ahead and save the file to the <code class="language-plaintext highlighter-rouge">uploads</code> folder. You <strong>must create a folder named <code class="language-plaintext highlighter-rouge">uploads</code> in the <code class="language-plaintext highlighter-rouge">wwwroot</code> folder of your application</strong>, otherwise, your code will throw an exception since the destination you’re asking it to save to doesn’t exist.</p>

<p>And there we have it! Compile, run, and record away! You can access the recorded files by visiting the <code class="language-plaintext highlighter-rouge">wwwroot/uploads</code> folder of your Blazor application.</p>

<h3 id="visualizing--the-audio-stream"><a name="visualizer">Visualizing </a> the audio stream</h3>

<p>Here is the code snippet for visualizing the audio stream. Add it to your <code class="language-plaintext highlighter-rouge">_Host.cshtml</code> file.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>let audioCtx;

// This function visualizes the audio stream coming out of the user's microphone.
// Credit: Soledad Penades of https://soledadpenades.com/ via https://mdn.github.io/web-dictaphone/

function visualize(stream, canvas, canvasCtx) {
    if (!audioCtx) {
        audioCtx = new AudioContext();
    }

    const source = audioCtx.createMediaStreamSource(stream);

    const analyser = audioCtx.createAnalyser();
    analyser.fftSize = 2048;
    const bufferLength = analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);

    source.connect(analyser);
    //analyser.connect(audioCtx.destination);

    draw()

    function draw() {
        const WIDTH = canvas.width
        const HEIGHT = canvas.height;

        requestAnimationFrame(draw);

        analyser.getByteTimeDomainData(dataArray);

        canvasCtx.fillStyle = 'rgb(200, 200, 200)';
        canvasCtx.fillRect(0, 0, WIDTH, HEIGHT);

        canvasCtx.lineWidth = 2;
        canvasCtx.strokeStyle = 'rgb(0, 0, 0)';

        canvasCtx.beginPath();

        let sliceWidth = WIDTH * 1.0 / bufferLength;
        let x = 0;


        for (let i = 0; i &lt; bufferLength; i++) {

            let v = dataArray[i] / 128.0;
            let y = v * HEIGHT / 2;

            if (i === 0) {
                canvasCtx.moveTo(x, y);
            } else {
                canvasCtx.lineTo(x, y);
            }

            x += sliceWidth;
        }

        canvasCtx.lineTo(canvas.width, canvas.height / 2);
        canvasCtx.stroke();

    }
}

</code></pre></div></div>

<p>Then make sure to uncomment</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>//visualize(stream, canvas, canvasCtx);

</code></pre></div></div>
<p>in the <a href="#onSuccess"><code class="language-plaintext highlighter-rouge">onSuccess</code></a> method. When you click record, you should now see a visualization of the audio stream. 
<br />
<br />
<br />
<br /></p>
<h3 id="github-source">Github Source</h3>
<p><a href="https://github.com/mbuotidem/BlazorAudioRecorder">BlazorAudioRecorder</a>
<br />
<br />
<br />
<br /></p>
<h3 id="further-reading">Further Reading</h3>

<p><a href="https://blog.addpipe.com/recording-audio-in-the-browser-using-pure-html5-and-minimal-javascript/">https://blog.addpipe.com/recording-audio-in-the-browser-using-pure-html5-and-minimal-javascript/</a></p>

<p><a href="https://blog.addpipe.com/using-recorder-js-to-capture-wav-audio-in-your-html5-web-site/">https://blog.addpipe.com/using-recorder-js-to-capture-wav-audio-in-your-html5-web-site/</a></p>

<p><a href="https://github.com/GersonRosales/Record-Audios-and-Videos-with-getUserMedia">https://github.com/GersonRosales/Record-Audios-and-Videos-with-getUserMedia</a></p>

<p><a href="https://developer.mozilla.org/en-US/docs/Web/API/MediaStream_Recording_API/Using_the_MediaStream_Recording_API">https://developer.mozilla.org/en-US/docs/Web/API/MediaStream_Recording_API/Using_the_MediaStream_Recording_API</a></p>

<p><a href="https://hacks.mozilla.org/2016/04/record-almost-everything-in-the-browser-with-mediarecorder/">https://hacks.mozilla.org/2016/04/record-almost-everything-in-the-browser-with-mediarecorder/</a></p>

<p><a href="https://mdn.github.io/web-dictaphone/">https://mdn.github.io/web-dictaphone/</a></p>

<p><a href="https://khalidabuhakmeh.com/upload-a-file-using-aspdotnet-core">https://khalidabuhakmeh.com/upload-a-file-using-aspdotnet-core</a></p>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="mbuotidem/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/2020/12/24/record-audio-in-blazor-using-mediarecorder-api-and-recorderjs.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Isaac Mbuotidem&#39;s Blog</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/mbuotidem" title="mbuotidem"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/mbuotidemisaac" title="mbuotidemisaac"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
