{
  
    
        "post0": {
            "title": "How I Studied For Clfc01 Aws Certified Cloud Practitioner Aws Ccp Exam",
            "content": "I just passed the CLF-C01 AWS Certified Cloud Practitioner Exam (AWS CCP) and like I did for the AZ-900: Microsoft Azure Fundamentals exam, I wanted to add some quick notes on the resources I used to prepare for it. . Before I continue, a note on cloud certifications and their value or lack of. For some reason, these certifications do seem attractive to some recruiters. This is perhaps because if nothing else, they indicate an individuals willingness to self-study and learn skills independently. However, if the ultimate goal is bolster ones hireability, I would not recommend pursuing these certifications without putting in the time to also truly use the services to build and deploy real applications. The winning combination for these certifications is having them in addition to work experience using the cloud services whether through an internship, volunteer position, or full-time job. If you don’t have any of those, then personal projects on GitHub that demonstrate their usage could stand in. Only then can one speak convincingly about the services and how they employed them to solve real-world problems. . Okay, now to the meat of the discussion. The AWS CCP is a beginner level AWS exam and although it has questionable value for a developer, I decided to take it anyway because doing so allows you to earn a 50% discount on a subsequent AWS Exam. I have my eyes on the AWS Certified Developer Associate, (DVA-C01) so getting this discount makes sense. This was especially important as I couldn’t find any other way to get a discount for AWS exams unlike is the case with Microsoft Azure certifications. . I prepped for this exam in a week using the following resources: . AWS Cloud Practitioner Essentials course available on Coursera. | AWS Cloud Practitioner Learning Path on Pluralsight | The Official AWS Services Overview Whitepaper | . If you don’t have much experience with the AWS, I would recommend supplementing the above with this free course courtesy of freeCodeCamp. . Next up, DVA-C01. .",
            "url": "https://mbuotidem.github.io/blog/2021/01/31/how-I-studied-for-CLFC01-AWS-Certified-Cloud-Practitioner-AWS-CCP-Exam.html",
            "relUrl": "/2021/01/31/how-I-studied-for-CLFC01-AWS-Certified-Cloud-Practitioner-AWS-CCP-Exam.html",
            "date": " • Jan 31, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "How I Studied For Az 900 Microsoft Azure Fundamentals Exam",
            "content": "I just passed the AZ-900: Microsoft Azure Fundamentals exam and I wanted to add some quick notes on the resources I used to prepare for it. Before I go on to do that, if you haven’t already, check out my post on how to score a free or discounted Azure certification exam. . Okay, back to the topic on hand, AZ-900. AZ-900 is a beginner level Azure exam and if you’ve used the Azure Portal to develop cloud applications, as was my case, passing it really does not require extensive preparation. All I used was the official training available from Microsoft Learn. The learning paths available here come with built-in free labs and links to Microsoft Documentation on the various Azure services discussed. The labs are a pretty nifty feature but not every service is accessible via them. For access to those services, consider signing up for a free Azure account. . If you don’t have much experience with the Azure Portal, I would recommend supplementing Microsoft Learn and the Microsoft Docs the various modules refer you to with some sort of course. For all things Microsoft, I lean towards Pluralsight. They do have an AZ-900 learning path If I had absolutely no Azure knowledge, this would have been my starting point. . Next up, AZ-204. .",
            "url": "https://mbuotidem.github.io/blog/2021/01/20/how-I-studied-for-AZ-900-microsoft-azure-fundamentals-exam.html",
            "relUrl": "/2021/01/20/how-I-studied-for-AZ-900-microsoft-azure-fundamentals-exam.html",
            "date": " • Jan 20, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "How To Get Free Or Discounted Microsoft Azure Certifications",
            "content": "The new year often brings with it resolutions. If you’re in tech, that might take the form of get a professional certification. If this is you and you’d like to do so without breaking the bank, here are some ideas for getting free or discounted certifications. . One way to obtain certiications is to attend events sponsored by the cloud provider. These include industry conferences like Microsoft Build which is actually how I got my first free exam voucher. I’d attended Build last year and participated in the Cloud Skills Challenge and was therefore eligible for a free certification. However, I’d completely forgotten about it until I got a reminder email from Microsoft that my free certification voucher was expiring. (BTW, I got this reminder email 21 days to the expiry date so don’t be like me and set your own reminder - 21 days might not be enough time to prepare for the cert you want!) . . And just like that, I am now signed up to take the AZ-900 Azure Fundamentals exam at the end of this month. Blog post incoming on my approach to preparing for this. . But what happens after AZ-900? How do I get more discounted certification vouchers? The answer is attend more Microsoft Events and take on more Microsoft Skills Challenges! . Microsoft offers Virtual Training Days where devs and IT professionals can learn more about their offerings. Many of these events will tell you in the description if there is a certification offer as in the image below: . . Your other option is to sign up and participate in Microsoft Cloud Skills Challenge - 30 Days to Learn It. As the name implies, you sign up to take a challenge to prepare for a specific Microsoft certification in 30 days. If you complete your training, you become eligible for 50 percent off the cost of the certification. Caveat: you can only get a discounted certification code once every 6 months. So what are you waiting for?! Go sign-up already so that you can snag two discounted certifications this year! You’ve got this! .",
            "url": "https://mbuotidem.github.io/blog/2021/01/17/how-to-get-free-or-discounted-microsoft-azure-certifications.html",
            "relUrl": "/2021/01/17/how-to-get-free-or-discounted-microsoft-azure-certifications.html",
            "date": " • Jan 17, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "How To Hot Reload Auto Refresh React App On Wsl",
            "content": "If you’re working on a React app on Windows using Windows Subsystem for Linux (WSL), you might find that your app does not reflect your changes on save. . Your first step to fixing this is to ensure that your React files are located on the WSL filesystem and not the Windows filesystem. For example, if your files are saved at : . C:Users yourusername Documents test-react-app . Use the cp command to copy them over to your WSL filesystem like so: . cp -r /mnt/c/Users/yourusername/Documents/test-react-app ~/test-react-app. . The above will copy the files to your WSL user account’s home directory which you can always get to by typing wsl ~ from Command Prompt or Windows Terminal. . If that still does not work, try running your app with: . $ CHOKIDAR_USEPOLLING=true npm start . Note however, that you will need to do this every time you want to start your app. . A more permanent solution is to create a .env file in your project directory if you don’t already have one and add . $ CHOKIDAR_USEPOLLING=true . to the .env file. . Credit : https://stackoverflow.com/a/56199112 .",
            "url": "https://mbuotidem.github.io/blog/2021/01/09/how-to-hot-reload-auto-refresh-react-app-on-WSL.html",
            "relUrl": "/2021/01/09/how-to-hot-reload-auto-refresh-react-app-on-WSL.html",
            "date": " • Jan 9, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "How To View Issues Commented On Github",
            "content": "Have you ever wanted to check back on a GitHub issue you made a comment on? I recently needed to and floundered about on the UI trying to find a link or button that would help me do that. Ultimately, I was unsuccessful and had to turn to Google. Here’s how to do it courtesy of the the lovely folks at stackexchange. . Make sure you’re logged in and then enter the url . https://github.com/notifications/subscriptions?commenter={yourusername} . So in my case, that would be : . https://github.com/notifications/subscriptions?commenter=mbuotidem .",
            "url": "https://mbuotidem.github.io/blog/2021/01/08/how-to-view-issues-commented-on-github.html",
            "relUrl": "/2021/01/08/how-to-view-issues-commented-on-github.html",
            "date": " • Jan 8, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Record Audio In Blazor Using Mediarecorder Api And Recorderjs",
            "content": "I recently started playing with Blazor and I decided to build an app that needed to get audio recorded from the users microphone via the browser. This post explains how I did that on Server Side Blazor. . I decided to go with using the browser’s MediaDevices interface in conjunction with Matt Diamond’s aptly named Recorder.js library. Although Recorder.js is no longer actively maintained, it still works and more importantly, lets me record in .wav format which was part of my requirement. . So how does all of this work in the context of a Blazor application? My approach boils down to the following: . Record the audio from the user’s browser using JavaScript | Post the recorded audio blob to an API endpoint on my Blazor Server App using XMLHttpRequest and FormData. | Save the audio blob to a file on disk. | Record the audio from the user’s browser using JavaScript and Recorder.js . As mentioned earlier, I’ll be using the MediaDevices API along with Recorder.js to record the audio from the browser. The MediaDevices API has a method, getUserMedia() which enables recording of all sorts of media streams - audio, video, screen share etc. In our case, we will be using just the audio recording capability via Recorder.js. More on this in a second. . The user interface . The UI for the audio recording is lifted straight from the web dictaphone sample. . . As the image above shows, we have a record and stop button along with an HTML5 canvas that visualizes the audio stream being heard by the microphone. Credit for the visualizer code goes to Soledad Penades. . Here is the Blazor component code for the UI above. I simply replaced the code that was in Index.razor but feel free to create and use a different component. . @page &quot;/&quot; @inject IJSRuntime jsRuntime &lt;div class=&quot;wrapper mt-5&quot;&gt; &lt;section class=&quot;main-controls&quot;&gt; &lt;canvas id=&quot;canvas&quot; class=&quot;visualizer&quot; height=&quot;60&quot;&gt;&lt;/canvas&gt; &lt;div id=&quot;buttons&quot;&gt; &lt;button class=&quot;@recordButton&quot; disabled=&quot;@Recording&quot; @onclick=Record&gt;Record&lt;/button&gt; &lt;button class=&quot;stop&quot; disabled=&quot;@NotRecording&quot; @onclick=Stop&gt;Stop&lt;/button&gt; &lt;/div&gt; &lt;/section&gt; &lt;section class=&quot;sound-clips&quot;&gt; &lt;/section&gt; &lt;audio controls autoplay&gt; &lt;/audio&gt; &lt;/div&gt; @code{ string recordButton = &quot;record&quot;; bool recording = false; bool notRecording = true; private async Task Record() { recordButton = &quot;recording&quot;; recording = true; notRecording = false; await jsRuntime.InvokeVoidAsync(&quot;MyJSMethods.startRecording&quot;); } private async Task Stop() { recordButton = &quot;record&quot;; recording = false; notRecording = true; await jsRuntime.InvokeVoidAsync(&quot;MyJSMethods.stopRecording&quot;); } } . The field recordButton toggles the class of the record button from ‘record’ to ‘recording’. We use this to change the record button’s color to red when a recording is in progress and back to blue when recording is stopped. . The fields ‘recording’ and ‘notRecording’ are boolean, and are used to enable and disable clicking on the record and stop buttons depending on if a recording is in progress. . The component’s methods are pretty simple. Record toggles our CSS property fields and then calls out to a JavaScript function startRecording via the IJSRuntime service that enables Blazor’s JavaScript interoperability. Stop toggles back the CSS fields and then calls the JavaScript function stopRecording to, well, stop recording. . Below is the CSS for the component which I placed in site.css. . #buttons { display: flex; flex-direction: row; justify-content: space-between; } #buttons button { font-size: 1rem; padding: 1rem; width: calc(50% - 0.25rem); } .record { background: #0088cc; text-align: center; color: white; } .record:hover, .record:focus { box-shadow: inset 0px 0px 10px rgba(255, 255, 255, 1); background: #0ae; } .record:active { box-shadow: inset 0px 0px 20px rgba(0,0,0,0.5); transform: translateY(2px); } .recording { background: red; text-align: center; color: white; } .stop { font-size: 1rem; background: #0088cc; text-align: center; color: white; border: none; transition: all 0.2s; padding: 0.5rem; } .stop:hover, .stop:focus { box-shadow: inset 0px 0px 10px rgba(255, 255, 255, 1); background: #0ae; } .stop:active { box-shadow: inset 0px 0px 20px rgba(0,0,0,0.5); transform: translateY(2px); } . Now that we understand how our Blazor component works, lets talk about the JavaScript part. . The JavaScript - getUserMedia and Recorder.js . Our first step is to add a link to Recorder.js in our _Host.cshtml page, right below the link to the Blazor server framework script. We also include an empty &lt;script&gt; tag that will contain our JavaScript. . &lt;script src=&quot;_framework/blazor.server.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://cdn.rawgit.com/mattdiamond/Recorderjs/08e7abd9/dist/recorder.js&quot;&gt;&lt;/script&gt; &lt;script&gt; &lt;/script&gt; . Next, we can start writing the JavaScript we need to record audio. Let’s start with the functions that are called from C#. Note that the rest of the JavaScript code in this post should be placed within the empty &lt;script&gt; tag above. . window.MyJSMethods = { startRecording: function () { navigator.getUserMedia({ audio: true }, onSuccess, onError); }, stopRecording: function (element) { stop.click(); }, } . startRecording above invokes the browser’s getUserMedia method which prompts the user to grant us access to their microphone in order to start recording audio. If that request succeeds, then we invoke the onSuccess method which is where the actual recording takes place. If the user refuses to grant us access, then we call onError. The nice thing about the way this works is that the user is prompted to grant access only once - the browser will remember that access was granted on subsequent visits to your page. Lets look at onError. . let onError = function (err) { console.log(&#39;The following error occurred: &#39; + err); }; . onError simply logs the error to the console. This was fine by me during development but is not very useful for the end user. Consider improving this to issue an alert and tell the user “Hey, you need to grant us access to your microphone if you want us to record your audio!”. Next up, onSuccess, where the recording magic happens! . let stop = document.querySelector(&#39;.stop&#39;); let onSuccess = function (stream) { let recorder; let context; let audio = document.querySelector(&#39;audio&#39;); stop.disabled = false; let mainSection = document.querySelector(&#39;.main-controls&#39;); const canvas = document.querySelector(&#39;.visualizer&#39;); canvas.width = mainSection.offsetWidth; const canvasCtx = canvas.getContext(&quot;2d&quot;); context = new AudioContext(); let mediaStreamSource = context.createMediaStreamSource(stream); recorder = new Recorder(mediaStreamSource); recorder.record(); //visualize(stream, canvas, canvasCtx); stop.onclick = function () { recorder.stop(); recorder.exportWAV(function (s) { wav = window.URL.createObjectURL(s); audio.src = window.URL.createObjectURL(s); let filename = new Date().toISOString().replaceAll(&#39;:&#39;, &quot;&quot;); let fd = new FormData(); fd.append(&quot;file&quot;, s, filename); let xhr = new XMLHttpRequest(); xhr.addEventListener(&quot;load&quot;, transferComplete); xhr.addEventListener(&quot;error&quot;, transferFailed) xhr.addEventListener(&quot;abort&quot;, transferFailed) xhr.open(&quot;POST&quot;, &quot;api/SaveAudio/Save/&quot;, true); xhr.send(fd); }); stop.disabled = true; function transferComplete(evt) { console.log(&quot;The transfer is complete.&quot;); //GLOBAL.DotNetReference.invokeMethodAsync(&#39;Recognize&#39;, filename); } function transferFailed(evt) { console.log(&quot;An error occurred while transferring the file.&quot;); console.log(evt.responseText); console.log(evt.status); } } } . onSuccess is called from getUserMedia which passes it a stream of the audio source. We then create the recorder, context, and audio variables. The API for Recorder.js is intuitive - to record, you call the record method on a Recorder object. A Recorder object takes a source and an optional config as parameters. In our case, we are using the browser’s AudioContext interface as our source. AudioContext processes the stream into an audio source that our Recorder instance can use when we invoke its record method. The audio variable just holds a reference to the HTML5 audio element through which we will play back the recorded audio to the user. . We also created two canvas related variables, canvas and canvasCtx, which we pass into a commented out call to the visualize function. This function handles the visualization of the audio stream and is not required for the recording to work. Ignore it for now - we will circle back to it. . Next is our stopping mechanism. Outside onSuccess, we have a handle on the stop button. To stop recording, we attach a function to the stop button’s onclick event which does a couple of things. First, it stops the recording and then uses Recorder.js’s exportWAV method to export the audio blob as a .wav file. When the file is ready, we create a filename based on the timestamp. We set this file as the source for our HTML5 audio element to allow for immediate playback of the recorded audio. Then we post the audio file to our backend via XMLHttpRequest. . Before we discuss how the file is posted to our backend, a quick word about the functions transferComplete and transferFailed. You can do whatever you want to based on the status of the transfer of the file via XMLHttpRequest by attaching event listeners to it. For example, in the actual application I was building, on a successful POST, I invoked a C# method, in the commented out line reproduced below: . //GLOBAL.DotNetReference.invokeMethodAsync(&#39;Recognize&#39;, filename); . This line triggered a method called Recognize in that version of my Blazor Component to perform speech recognition on the recorded audio file. To learn more about calling C# code from JavaScript via the Blazor JavaScript interop, see here and here. . Post the recorded audio blob to an API endpoint on my Blazor Server App using XMLHttpRequest and FormData . Posting the audio blob is fairly straight forward. As mentioned earlier, we use the XMLHttpRequest and FormData objects. Here’s that portion of the onSuccess function again: . recorder.exportWAV(function (s) { wav = window.URL.createObjectURL(s); audio.src = window.URL.createObjectURL(s); let filename = new Date().toISOString().replaceAll(&#39;:&#39;, &quot;&quot;); let fd = new FormData(); fd.append(&quot;file&quot;, s, filename); let xhr = new XMLHttpRequest(); xhr.addEventListener(&quot;load&quot;, transferComplete); xhr.addEventListener(&quot;error&quot;, transferFailed) xhr.addEventListener(&quot;abort&quot;, transferFailed) xhr.open(&quot;POST&quot;, &quot;api/SaveAudio/Save/&quot;, true); xhr.send(fd); }); . First we create the FormData object and then use its append method to add the filename we created as well as the recorded .wav file, here represented by s, to the form. Then we create the XMLHttpRequest object and attach event listeners for a successful transfer, as indicated by load, as well as for failures such as error and abort. . Next, we initialize a POST request with XMLHttpRequest’s open method, indicating the API endpoint url api/SaveAudio/Save as the target. And then we invoke the send method, performing the actual POST request with the FormData object containing our audio blob file as its payload. Note that if your API is on a different domain, you might need to take steps to resolve CORS issues. . Setup required to create the .NET Web API endpoint . To add an API endpoint to our Blazor Server application, we need to make some changes to the Startup.cs class. Add the using statement for .Net Core MVC to the list of using statements. . using Microsoft.AspNetCore.Mvc; . Then modify the ConfigureServices method, adding the Mvc service: . public void ConfigureServices(IServiceCollection services) { services.AddMvc(options =&gt; options.EnableEndpointRouting = false).SetCompatibilityVersion(CompatibilityVersion.Latest); ... } . Next, add app.UseMvcWithDefaultRoute(); to the Configure method. I placed it right after app.UseRouting() and before app.UseEndpoints(); . public void Configure(IApplicationBuilder app, IWebHostEnvironment env) { ... app.UseRouting(); app.UseMvcWithDefaultRoute(); app.UseEndpoints(endpoints =&gt; { endpoints.MapControllers(); endpoints.MapBlazorHub(); endpoints.MapFallbackToPage(&quot;/_Host&quot;); }); } . With this setup in place, we can now create our API controller class. . Save the audio blob to a file on disk. . To receive the audio, we will be creating an ASP.NET Core Controller with a method Save. The class itself is called SaveAudio. Together, these map to the the API endpoint api/SaveAudio/Save which we used earlier in the JavaScript code. To add this route to our controller, we use the attribute route [Route(&quot;api/[controller]/Save&quot;)]. Below is the code. . using Microsoft.AspNetCore.Http; using Microsoft.AspNetCore.Mvc; using System.IO; using System.Threading.Tasks; namespace BlazorAudioRecorder { public class SaveAudio: Controller { Microsoft.AspNetCore.Hosting.IWebHostEnvironment _hostingEnvironment; public SaveAudio(Microsoft.AspNetCore.Hosting.IWebHostEnvironment hostingEnvironment) { _hostingEnvironment = hostingEnvironment; } [Route(&quot;api/[controller]/Save&quot;)] [HttpPost] public async Task&lt;IActionResult&gt; Save(IFormFile file) { if (file.ContentType != &quot;audio/wav&quot;) { return BadRequest(&quot;Wrong file type&quot;); } var uploads = Path.Combine(_hostingEnvironment.WebRootPath, &quot;uploads&quot;); var filePath = Path.Combine(uploads, file.FileName + &quot;.wav&quot;); using (var fileStream = new FileStream(filePath, FileMode.Create)) { await file.CopyToAsync(fileStream); } return Ok(&quot;File uploaded successfully&quot;); } } } . The Save method will be invoked when an HttpPost request is made to the endpoint api/SaveAudio/Save. Notice that the name of the IFormFile parameter is file which corresponds to the name we gave the audio blob when creating our FormData object earlier. Here is that specific line of JavaScript code again: . fd.append(&quot;file&quot;, s, filename); . By doing this, we can rely on ASP.NET model binding magic to match everything up for us. The rest of the code is fairly simple, we check if the content type matches our expectation of audio/wav. If it doesn’t, we reject it, but if does, we go ahead and save the file to the uploads folder. You must create a folder named uploads in the wwwroot folder of your application, otherwise, your code will throw an exception since the destination you’re asking it to save to doesn’t exist. . And there we have it! Compile, run, and record away! You can access the recorded files by visiting the wwwroot/uploads folder of your Blazor application. . Visualizing the audio stream . Here is the code snippet for visualizing the audio stream. Add it to your _Host.cshtml file. . let audioCtx; // This function visualizes the audio stream coming out of the user&#39;s microphone. // Credit: Soledad Penades of https://soledadpenades.com/ via https://mdn.github.io/web-dictaphone/ function visualize(stream, canvas, canvasCtx) { if (!audioCtx) { audioCtx = new AudioContext(); } const source = audioCtx.createMediaStreamSource(stream); const analyser = audioCtx.createAnalyser(); analyser.fftSize = 2048; const bufferLength = analyser.frequencyBinCount; const dataArray = new Uint8Array(bufferLength); source.connect(analyser); //analyser.connect(audioCtx.destination); draw() function draw() { const WIDTH = canvas.width const HEIGHT = canvas.height; requestAnimationFrame(draw); analyser.getByteTimeDomainData(dataArray); canvasCtx.fillStyle = &#39;rgb(200, 200, 200)&#39;; canvasCtx.fillRect(0, 0, WIDTH, HEIGHT); canvasCtx.lineWidth = 2; canvasCtx.strokeStyle = &#39;rgb(0, 0, 0)&#39;; canvasCtx.beginPath(); let sliceWidth = WIDTH * 1.0 / bufferLength; let x = 0; for (let i = 0; i &lt; bufferLength; i++) { let v = dataArray[i] / 128.0; let y = v * HEIGHT / 2; if (i === 0) { canvasCtx.moveTo(x, y); } else { canvasCtx.lineTo(x, y); } x += sliceWidth; } canvasCtx.lineTo(canvas.width, canvas.height / 2); canvasCtx.stroke(); } } . Then make sure to uncomment . //visualize(stream, canvas, canvasCtx); . in the onSuccess method. When you click record, you should now see a visualization of the audio stream. . Github Source . BlazorAudioRecorder . Further Reading . https://blog.addpipe.com/recording-audio-in-the-browser-using-pure-html5-and-minimal-javascript/ . https://blog.addpipe.com/using-recorder-js-to-capture-wav-audio-in-your-html5-web-site/ . https://github.com/GersonRosales/Record-Audios-and-Videos-with-getUserMedia . https://developer.mozilla.org/en-US/docs/Web/API/MediaStream_Recording_API/Using_the_MediaStream_Recording_API . https://hacks.mozilla.org/2016/04/record-almost-everything-in-the-browser-with-mediarecorder/ . https://mdn.github.io/web-dictaphone/ . https://khalidabuhakmeh.com/upload-a-file-using-aspdotnet-core .",
            "url": "https://mbuotidem.github.io/blog/2020/12/24/record-audio-in-blazor-using-mediarecorder-api-and-recorderjs.html",
            "relUrl": "/2020/12/24/record-audio-in-blazor-using-mediarecorder-api-and-recorderjs.html",
            "date": " • Dec 24, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Error Connect Enetunreach 169.254.169.254.80",
            "content": "When you are using the AWS Node.js SDK, you might encouter the error message : . UnhandledPromiseRejectionWarning: Error: connect ENETUNREACH 169.254.169.254:80 at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1145:16). . This error is likely occuring because the AWS SDK is unable to find your AWS credentials. AWS Documentation has a list of ways you can load credentials in Node.js . Where the credential files are stored depends on your operating system. . on Linux, Unix, and macOS: ~/.aws/credentials . | on Windows: C: Users USER_NAME .aws credentials . | . In my case, the issue was that I did not have a [default] profile in my aws credentials file. As such, I needed to tell the AWS Node.js SDK which profile to use instead. . Assuming your profile is named `myspecial-profile, here’s how you can do that : . const AWS = require(&#39;aws-sdk&#39;); AWS.config.region = &#39;us-east-1&#39;; var credentials = new AWS.SharedIniFileCredentials({profile: &#39;myspecial-profile&#39;}); AWS.config.credentials = credentials; . Learn more about loading AWS credentials in Node.js from the shared credentials file. .",
            "url": "https://mbuotidem.github.io/blog/2020/12/19/Error-connect-ENETUNREACH-169.254.169.254.80.html",
            "relUrl": "/2020/12/19/Error-connect-ENETUNREACH-169.254.169.254.80.html",
            "date": " • Dec 19, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Venv In Vscode Integrated Terminal",
            "content": "When you choose to use a python venv virtual environment in vscode, you might run into the error below : . The file C: yourpath .env Scripts Activate.ps1 is not digitally signed. You cannot run this script on the current system. . . A quick solution is to add this to your vscode settings.json file : . { &quot;terminal.integrated.shellArgs.windows&quot;: [ &quot;-ExecutionPolicy&quot;, &quot;Bypass&quot; ], } . You should restart your vscode and you’ll know you succeeded if you see the popup shown in the image below. Click allow and restart vscode again and you should be able to start your venv virtual environment without issues. . . Credit : https://stackoverflow.com/a/56199112 .",
            "url": "https://mbuotidem.github.io/blog/2020/10/09/venv-in-vscode-integrated-terminal.html",
            "relUrl": "/2020/10/09/venv-in-vscode-integrated-terminal.html",
            "date": " • Oct 9, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://mbuotidem.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://mbuotidem.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a software engineer who loves working with cloud applications. In my current role, I am spending a lot of time in the AWS ecosystem building serverless backends with Python. In the past, I have worked with C#, the Microsoft stack, and Azure, doing everything from developing web applications to chatbots. I am most comfortable working in Python, C# and JavaScript on both AWS and Azure. . This website is powered by fastpages 1. .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://mbuotidem.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mbuotidem.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}